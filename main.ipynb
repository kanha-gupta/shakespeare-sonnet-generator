{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dFZoy1luHhyw",
    "outputId": "3d0d3f56-603c-47f1-875c-27e0ba0b4d7d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/15, Loss: 1.9069862222444212\n",
      "Epoch 2/15, Loss: 1.5635372025243353\n",
      "Epoch 3/15, Loss: 1.4643457078349238\n",
      "Epoch 4/15, Loss: 1.409766277367459\n",
      "Epoch 5/15, Loss: 1.3736680722131103\n",
      "Epoch 6/15, Loss: 1.3480362291030832\n",
      "Epoch 7/15, Loss: 1.3283089159863881\n",
      "Epoch 8/15, Loss: 1.3126506206735202\n",
      "Epoch 9/15, Loss: 1.2991797537636425\n",
      "Epoch 10/15, Loss: 1.28810113374173\n",
      "Epoch 11/15, Loss: 1.2784171856541515\n",
      "Epoch 12/15, Loss: 1.2704241371982796\n",
      "Epoch 13/15, Loss: 1.2630448445276943\n",
      "Epoch 14/15, Loss: 1.2565489055187393\n",
      "Epoch 15/15, Loss: 1.250594989962021\n",
      "Shall I compare thee to a summer's day?o\n",
      "Where thou art this thou art thou art thou art still,\n",
      "And to the stand the stars to these stands that thee still,\n",
      "And therefore to thee, thou art thou art thou shalt store,\n",
      "And then thou these stars to these stand to thee,\n",
      "And thou art thou art those this to the stars\n",
      "And that thou that thou art thine eyes of truth,\n",
      "And then thou art the stars to these standed stay\n",
      "The stars to these stands that\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "with open(\"Sonnets.txt\", \"r\") as file:\n",
    "    sonnets_text = file.read()\n",
    "\n",
    "# Tokenize the sonnets at the character level\n",
    "chars = sorted(list(set(sonnets_text)))\n",
    "char_to_idx = {char: i for i, char in enumerate(chars)}\n",
    "idx_to_char = {i: char for i, char in enumerate(chars)}\n",
    "char_sequence = [char_to_idx[char] for char in sonnets_text]\n",
    "\n",
    "# Define a character-based dataset\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequence, sequence_length):\n",
    "        self.sequence = sequence\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.sequence[index:index+self.sequence_length]),\n",
    "            torch.tensor(self.sequence[index+1:index+self.sequence_length+1])\n",
    "        )\n",
    "\n",
    "sequence_length = 100\n",
    "char_dataset = CharDataset(char_sequence, sequence_length)\n",
    "char_dataloader = DataLoader(char_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the LSTM model\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.fc(self.dropout(x))\n",
    "        return x, hidden\n",
    "\n",
    "# Hyperparameters\n",
    "char_vocab_size = len(chars)\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "output_dim = char_vocab_size\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "char_model = CharLSTM(char_vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout)\n",
    "char_criterion = nn.CrossEntropyLoss()\n",
    "char_optimizer = torch.optim.Adam(char_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 15\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for inputs, targets in char_dataloader:\n",
    "        batch_size = inputs.size(0)  # Corrected placement\n",
    "        hidden = (torch.zeros(n_layers, batch_size, hidden_dim), torch.zeros(n_layers, batch_size, hidden_dim))\n",
    "\n",
    "        char_optimizer.zero_grad()\n",
    "        outputs, hidden = char_model(inputs, hidden)\n",
    "        hidden = (hidden[0].detach(), hidden[1].detach())\n",
    "        outputs = outputs.view(-1, outputs.shape[2])\n",
    "        targets = targets.view(-1)\n",
    "        loss = char_criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        char_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {total_loss/len(char_dataloader)}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(char_model.state_dict(), 'char_lstm_model2.pth')\n",
    "\n",
    "# To generate text after training:\n",
    "def generate_text(model, start_string, generate_length=100):\n",
    "    model.eval()\n",
    "    text_generated = []\n",
    "    input_eval = torch.tensor([char_to_idx[s] for s in start_string]).unsqueeze(0)\n",
    "    hidden = (torch.zeros(n_layers, 1, hidden_dim), torch.zeros(n_layers, 1, hidden_dim))\n",
    "\n",
    "    for i in range(generate_length):\n",
    "        outputs, hidden = model(input_eval, hidden)\n",
    "        predicted_id = torch.argmax(outputs, dim=2)[-1, 0].item()\n",
    "        input_eval = torch.tensor([[predicted_id]])\n",
    "        text_generated.append(idx_to_char[predicted_id])\n",
    "\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "# Generate a sonnet\n",
    "new_sonnet = generate_text(char_model, start_string=\"Shall I compare thee to a summer's day?\", generate_length=400)\n",
    "print(new_sonnet)\n"
   ]
  }
 ]
}
